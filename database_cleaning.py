# -*- coding: utf-8 -*-
"""Database_cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hRYm0TThE0EGySwoCoCAupxh2V-rzEk5

# Data Cleaning Jupyter Notebook
"""

# Import dependencies.
import pandas as pd
import numpy as np

"""## 1. olist_customers_dataset.csv"""

# Create new dataframe from olist_customers_dataset.csv
customer_data = pd.read_csv("Data_manupriya/olist_customers_dataset.csv.zip")

# Set index for dataframe as customer_id.
customer_data.set_index('customer_id', inplace=True)

# Print dataframe.
customer_data.head()

# Count rows of each column in customer_data.
customer_data.count()

# Check for null values.
customer_data.isnull().sum()

# Check for number of unique entries.
customer_data.nunique()

# Drop duplicates from customer_unique_id.
customer_data.drop_duplicates(subset = ['customer_unique_id'], inplace = True)

# Count rows in each column.
customer_data.count()

# Check datatypes of each column.
customer_data.dtypes

"""## 2.  olist_geolocation_dataset.csv"""

# Create new dataframe from olist_geolocation_dataset.csv.
geolocation_data = pd.read_csv("Data_manupriya/olist_geolocation_dataset.csv.zip")

# Set index as geolocation_zip_code_prefix.
geolocation_data.set_index("geolocation_zip_code_prefix", inplace = True)
geolocation_data.head()

# Count number of entries.
geolocation_data.count()

# Check for null values.
geolocation_data.isnull().sum()

# Check datatypes.
geolocation_data.dtypes

"""## 3.  olist_order_items_dataset.csv"""

# Create pandas dataframe for olist_order_items_dataset.csv and use parse_dates to infer datetime format on
# shipping_limit_date column.
parse_dates = ["shipping_limit_date"]
order_item_data = pd.read_csv("Data_manupriya/olist_order_items_dataset.csv.zip",\
                              infer_datetime_format = True, parse_dates = parse_dates)

# Reset index and check dataframe.
order_item_data.reset_index(drop = False , inplace = True)
order_item_data.head()

# Drop duplicates from order_id.
order_item_data.drop_duplicates(subset = ['order_id'], inplace = True)

# Count rows in each column.
order_item_data.count()

# Set index as order_id and check dataframe.
order_item_data.set_index("order_id", inplace = True)
order_item_data.head()

# Check for number of uniques values in each column.
order_item_data.nunique()

# Check for null values.
order_item_data.isnull().sum()

# Check datatypes in each column.
order_item_data.dtypes

order_item_data['shipping_limit_date'] = order_item_data['shipping_limit_date'].dt.date
order_item_data.head()

order_item_data['shipping_limit_date'] = pd.to_datetime(order_item_data['shipping_limit_date'])

# Drop index column.
order_item_data.drop(columns = ["index"], inplace = True)
order_item_data.head()

# Check datatypes of each column.
order_item_data.dtypes

"""## 4. olist_order_payments_dataset.csv"""

# Read in olist_order_payments_dataset.csv and make pandas dataframe.
order_payment_data = pd.read_csv("Data_manupriya/olist_order_payments_dataset.csv.zip")

# Drop duplicates from order_id column.
order_payment_data.drop_duplicates(subset = ['order_id'], inplace = True)

# Set index as order_id.
order_payment_data.set_index("order_id", inplace = True)

# Print dataframe.
order_payment_data.head()

# Count rows of each column.
order_payment_data.count()

# Count number of unique entries.
order_payment_data.nunique()

# Print dataframe.
order_payment_data.head()

# Check for null values.
order_payment_data.isnull().sum()

"""## 5. olist_order_reviews_dataset.csv"""

# Read in olist_order_reviews_dataset.csv as pandas dataframe and parse dates of
# "review_creation_date" and "review_answer_timestamp" columns.
parse_dates = ["review_creation_date", "review_answer_timestamp"]
order_reviews_data = pd.read_csv("Data_manupriya/olist_order_reviews_dataset.csv.zip",\
                                 infer_datetime_format = True, parse_dates = parse_dates)

# Drop duplicates from review_id column.
order_reviews_data.drop_duplicates(subset = ['review_id'], inplace = True)

# Set index as review_id column.
order_reviews_data.set_index("review_id", inplace = True)

# Print dataframe.
order_reviews_data.head()

# Count number of entries in each column.
order_reviews_data.count()

# Count number of unique entries.
order_reviews_data.nunique()

# Check for null values.
order_reviews_data.isnull().sum()

# Drop columns with null values.
order_reviews_data.drop(columns = ["review_comment_title", "review_comment_message"], inplace = True)

# Check datatypes.
order_reviews_data.dtypes

# Change column to datetime.
order_reviews_data['review_answer_timestamp'] = order_reviews_data['review_answer_timestamp'].dt.date
order_reviews_data.head()

# Check datatypes.
order_reviews_data.dtypes

# Change column to datetime.
order_reviews_data['review_answer_timestamp'] = pd.to_datetime(order_reviews_data['review_answer_timestamp'])

# Check datatypes.
order_reviews_data.dtypes

"""## 6. olist_orders_dataset.csv"""

# Read in olist_orders_dataset.csv as a pandas dataframe.
parse_dates = ["order_purchase_timestamp", "order_approved_at", "order_delivered_carrier_date", "order_delivered_customer_date", "order_estimated_delivery_date"]
orders_data = pd.read_csv("Data_manupriya/olist_orders_dataset.csv.zip", infer_datetime_format = True, parse_dates = parse_dates)

# Set index as order_id.
orders_data.set_index("order_id", inplace = True)

# Print dataframe.
orders_data.head()

# Count number of entries in each column.
orders_data.count()

# Count number of unique entries in each column.
orders_data.nunique()

# Check for null values.
orders_data.isnull().sum()

orders_data["order_purchase_timestamp"] = orders_data["order_purchase_timestamp"].dt.date
orders_data["order_approved_at"] = orders_data["order_approved_at"].dt.date
orders_data["order_delivered_carrier_date"] = orders_data["order_delivered_carrier_date"].dt.date
orders_data["order_delivered_customer_date"] = orders_data["order_delivered_customer_date"].dt.date

# Check datatypes of each column.
orders_data.dtypes

# Drop null values.
orders_data.dropna(axis = 0, how = "any", inplace = True)

#Check for null values.
orders_data.isnull().sum()

# Change datatypes of certain columns to datetime.
orders_data['order_purchase_timestamp'] = pd.to_datetime(orders_data['order_purchase_timestamp'])
orders_data['order_approved_at'] = pd.to_datetime(orders_data['order_approved_at'])
orders_data['order_delivered_carrier_date'] = pd.to_datetime(orders_data['order_delivered_carrier_date'])
orders_data['order_delivered_customer_date'] = pd.to_datetime(orders_data['order_delivered_customer_date'])

# Check datatypes of each column.
orders_data.dtypes

"""## 7. olist_products_dataset.csv"""

# Read in olist_products_dataset.csv as pandas dataframe.
product_data = pd.read_csv("Data_manupriya/olist_products_dataset.csv.zip")

# Drop columns.
product_data.drop(columns = ['product_weight_g', "product_length_cm", 'product_height_cm', 'product_width_cm', "product_name_lenght","product_description_lenght"], inplace = True)

# Drop null values.
product_data.dropna(axis = 0, how = "any", inplace = True)

# Set index to product_id.
product_data.set_index("product_id", inplace = True)
product_data.head()

# Count number of entries in each column.
product_data.count()

# Count number of unique values in each column.
product_data.nunique()

# Check for null values.
product_data.isnull().sum()

# Replace "other" with 0 in product_photos_qty column.
product_data['product_photos_qty'].replace("other", 0, inplace = True)

# Print dataframe.
product_data.head()

"""## 8. olist_sellers_dataset.csv"""

# Read in olist_sellers_dataset.csv as pandas dataframe.
seller_data = pd.read_csv("Data_manupriya/olist_sellers_dataset.csv")

# Set index as seller_id.
seller_data.set_index("seller_id",  inplace = True)
seller_data.head()

# Count number of rows in each column.
seller_data.count()

# Count number of unique entries per column.
seller_data.nunique()

# Count number of null values.
seller_data.isna().sum()

"""## 9. product_category_name_translation.csv"""

# Read in product_category_name_translation.csv as pandas dataframe.
product_catagory_data = pd.read_csv("Data_manupriya/product_category_name_translation.csv")

# Drop duplicates in product_category_name column.
product_catagory_data.drop_duplicates(subset = ['product_category_name'], inplace = True)

# Set index as product_category_name.
product_catagory_data.set_index("product_category_name", inplace = True)
product_catagory_data.head()

# Count number of rows.
product_catagory_data.count()

# Count number of uniques entries.
product_catagory_data.nunique()

# Count number of null values.
product_catagory_data.isna().sum()

"""## Export cleaned dataframes as CSV files"""

customer_data.to_csv("final_database/customer_data.csv")

geolocation_data.to_csv("final_database/geolocation_data.csv")

order_item_data.to_csv("final_database/order_item_data.csv")

order_payment_data.to_csv("final_database/order_payment_data.csv")

order_reviews_data.to_csv("final_database/order_reviews_data.csv")

product_data.to_csv("final_database/product_data.csv")

seller_data.to_csv("final_database/seller_data.csv")

product_catagory_data.to_csv("final_database/product_catagory_data.csv")

orders_data.to_csv("final_database/orders_data.csv")